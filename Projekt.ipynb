{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code is heavily inspired by these projects:\n",
    "- https://github.com/mathematiguy/keras-char-rnn\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://www.kaggle.com/mrisdal/intro-to-lstms-w-keras-gpu-for-text-generation/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import slabikar\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import keract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 120 #length of sequence\n",
    "step = 13 #overlap\n",
    "validation_split = 0.1\n",
    "batch_size = 128\n",
    "rnn_size = 1#128\n",
    "num_layers = 2\n",
    "drop_prob = 0.1\n",
    "epochs = 6\n",
    "temperature=1.0\n",
    "sample_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Function for concatenating all text files from directory. Text files are expected to be utf-8 encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file: Peklo.txt\n",
      "loading file: Nebo.txt\n",
      "loading file: Ocistec.txt\n"
     ]
    }
   ],
   "source": [
    "text_data = ''\n",
    "for filename in filter(lambda s: s.endswith(\".txt\"), os.listdir('resources/')):\n",
    "    # open file with default encoding\n",
    "    print(\"loading file: %s\" % filename)\n",
    "    filepath = os.path.join('resources/', filename)\n",
    "    with open(filepath,'r', encoding='utf-8') as f:\n",
    "        text_data += f.read() + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is pretty small, we convert everything to lowercase and remove diacritics. There are some characters that need to be tweaked beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = text_data.replace(\"’\",\"\\'\")\n",
    "text_data = text_data.replace(\"„\",\"\\\"\")\n",
    "text_data = text_data.replace(\"“\",\"\\\"\")\n",
    "text_data = text_data.replace(\"‒\",\"-\")\n",
    "import unidecode\n",
    "text_data = unidecode.unidecode(text_data).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for processing texts. One uses syllables as text atoms, the other uses characters\n",
    "\n",
    "Return values:\n",
    "- atom_to_int: (dict) Maps characters in the character set to ints.\n",
    "- int_to_atom: (dict) Maps ints to characters in the character set.\n",
    "- n_atom: (int) The number of characters in the text.\n",
    "- n_vocab: (int) The number of unique characters in the text.'''\n",
    "- data: preprocessed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_char(text_data):\n",
    "    # create mapping of unique chars to integers, and a reverse mapping\n",
    "    chars = sorted(set(text_data)) #sorted is necessary for checkpointing model \n",
    "    char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "    int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(text_data)\n",
    "    n_vocab = len(chars)    \n",
    "    return char_to_int, int_to_char, n_chars, n_vocab, text_data\n",
    "\n",
    "def process_text_syllable(text_data):\n",
    "    syllable_data = slabikar.slabikar(text_data)\n",
    "    syllables = sorted(set(syllable_data))\n",
    "    syllable_to_int = {c: i for i, c in enumerate(syllables)}\n",
    "    int_to_syllable = {i: c for i, c in enumerate(syllables)}\n",
    "    # summarize the loaded data\n",
    "    n_syllables = len(syllable_data)\n",
    "    n_vocab = len(syllables)    \n",
    "    return syllable_to_int, int_to_syllable, n_syllables, n_vocab, syllable_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes data to overlapping sequences. Targets are single atoms. Syllable data cannot be preprocessed as whole. Use generator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInput(text, maxlen, step, n_vocab, atom_to_int):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(text) - maxlen - 1, step):\n",
    "        seq_in = text[i: i + maxlen]\n",
    "        seq_out = text[i+maxlen] #(text[i + 1: i + maxlen + 1]) #weird\n",
    "        dataX.append([atom_to_int[atom] for atom in seq_in])\n",
    "        dataY.append(atom_to_int[seq_out])\n",
    "    #should one hot encode\n",
    "    print(len(dataX))\n",
    "    print(len(dataY))\n",
    "    print(len(dataX[0]))\n",
    "    X = np_utils.to_categorical(dataX, num_classes=n_vocab)\n",
    "    y = np_utils.to_categorical(dataY, num_classes=n_vocab)\n",
    "    #same thing as\n",
    "    #X = np.zeros((len(sentences), maxlen, n_vocab), dtype=np.bool)\n",
    "    #y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "    #for i in range(len(sentences)):\n",
    "    #    sentence = sentences[i]\n",
    "    #    target = targets[i]\n",
    "    #    for j in range(maxlen):\n",
    "    #        X[i][j][atom_to_int[sentence[j]]] = 1\n",
    "    #    y[i][atom_to_int[target[j]]] = 1\n",
    "    return X,y\n",
    "\n",
    "def get_batch(batch, starts, text_data, seq_length, batch_size, \n",
    "              atom_to_int, n_vocab):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for start in range(batch_size * batch, batch_size * (batch + 1)): \n",
    "        seq_in  = text_data[starts[start]:starts[start] + seq_length]\n",
    "        seq_out = text_data[starts[start] + seq_length]\n",
    "        dataX.append([atom_to_int[atom] for atom in seq_in])\n",
    "        dataY.append(atom_to_int[seq_out])\n",
    "        \n",
    "    X = np_utils.to_categorical(dataX, num_classes=n_vocab)\n",
    "    X = X.reshape(batch_size, seq_length, n_vocab) #necessary<\n",
    "    y = np_utils.to_categorical(dataY, num_classes=n_vocab)\n",
    "    return X, y\n",
    "\n",
    "#use this for syllables\n",
    "def generate_batches(mode, text_data, seq_length, validation_split,\n",
    "                     batch_size, atom_to_int, n_atom, n_vocab,\n",
    "                     random_seed=42, shuffle=True):#rovnaky seed zabezpeci rovnakost pre oba gneratory\n",
    "    random.seed(random_seed)\n",
    "    # index the text_data\n",
    "    starts = list(range(n_atom - n_atom % seq_length - seq_length))\n",
    "    if shuffle:#internal state?\n",
    "        # shuffle the indices\n",
    "        random.shuffle(starts)\n",
    "    n_batches = n_atom // batch_size\n",
    "    validation_size = round(n_batches * validation_split)\n",
    "    while True:\n",
    "        if mode == 'validation':\n",
    "            for batch in range(validation_size):\n",
    "                X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                                 batch_size, atom_to_int, n_vocab)\n",
    "                yield X, y\n",
    "            \n",
    "        elif mode == 'train':\n",
    "            for batch in range(validation_size, n_batches-1):\n",
    "                X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                                 batch_size, atom_to_int, n_vocab)\n",
    "                yield X, y\n",
    "        else:\n",
    "            raise ValueError(\"only 'validation' and 'train' modes accepted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size, seq_length, n_vocab, rnn_size, num_layers, drop_prob):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            # add last hidden layer\n",
    "            model.add(LSTM(rnn_size, return_sequences=False))\n",
    "            #model.add(TimeDistributed(Dense(num_chars))) #what is better?\n",
    "        elif i == 0:\n",
    "            # add first hidden layer\n",
    "            model.add(LSTM(rnn_size, batch_input_shape=(None, seq_length, n_vocab), return_sequences=True))\n",
    "        else:\n",
    "            # add middle hidden layer\n",
    "            model.add(LSTM(rnn_size, return_sequences=True))\n",
    "        \n",
    "        model.add(Dropout(drop_prob))\n",
    "    # add output layer\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback for printing while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked for specified epochs. Prints generated text.\n",
    "    # Using epoch+1 to be consistent with the training epochs printed by Keras\n",
    "    if epoch % 5 == 0:\n",
    "        print()\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "        #dirty global hack\n",
    "        start_index = random.randint(0, n_atoms - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = data[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, n_vocab))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, atom_to_int[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = int_to_atom[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n",
    "    else:\n",
    "        print()\n",
    "        print('----- Not generating text after Epoch: %d' % epoch)\n",
    "\n",
    "generate_text = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use this to prepare char data (contains some debug printing to ensure everything looks good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40438\n",
      "40438\n",
      "120\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, ':': 10, ';': 11, '?': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'q': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'v': 34, 'w': 35, 'x': 36, 'y': 37, 'z': 38}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: ':', 11: ';', 12: '?', 13: 'a', 14: 'b', 15: 'c', 16: 'd', 17: 'e', 18: 'f', 19: 'g', 20: 'h', 21: 'i', 22: 'j', 23: 'k', 24: 'l', 25: 'm', 26: 'n', 27: 'o', 28: 'p', 29: 'q', 30: 'r', 31: 's', 32: 't', 33: 'u', 34: 'v', 35: 'w', 36: 'x', 37: 'y', 38: 'z'}\n",
      "525809\n",
      "39\n",
      "40438\n",
      "40438\n"
     ]
    }
   ],
   "source": [
    "atom_to_int, int_to_atom, n_atoms, n_vocab, data = process_text_char(text_data)\n",
    "X,y = createInput(data, maxlen, step, n_vocab, atom_to_int)\n",
    "print(atom_to_int)\n",
    "print(int_to_atom)\n",
    "print(n_atoms)\n",
    "print(n_vocab)\n",
    "print(len(y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36394 samples, validate on 4044 samples\n",
      "Epoch 1/6\n",
      "27904/36394 [======================>.......] - ETA: 23s - loss: 3.5314 - acc: 0.1513"
     ]
    }
   ],
   "source": [
    "callbacks = [generate_text,ModelCheckpoint('checkpoints/weights-{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
    "model = build_model(batch_size, maxlen, n_vocab, rnn_size, num_layers, drop_prob)\n",
    "model.fit(X,y,batch_size=batch_size,epochs=epochs,callbacks=callbacks, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to prepare syllable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atom_to_int, int_to_atom, n_atoms, n_vocab, data = process_text_syllable(text_data)\n",
    "#print(atom_to_int)\n",
    "#print(int_to_atom)\n",
    "print(n_atoms)\n",
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train syllable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [generate_text,ModelCheckpoint('checkpoints/weights-{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
    "model = build_model(batch_size, maxlen, n_vocab, rnn_size, num_layers, drop_prob)\n",
    "\n",
    "n_batches = len(data) // batch_size\n",
    "batch_params = (data, maxlen, validation_split, batch_size, atom_to_int, n_atoms, n_vocab)\n",
    "model.fit_generator(\n",
    "    generator = generate_batches('train', *batch_params),\n",
    "    validation_data = generate_batches('validation', *batch_params),\n",
    "    validation_steps = int(n_batches * validation_split),\n",
    "    epochs = epochs,\n",
    "    steps_per_epoch = n_batches-1,\n",
    "    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can load saved model to play around with it (make sure to use correct preprocessing for loaded model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('.h5')#specify the file you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is used to generate some sample outputs from trained model. It also saves activations of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_atom_index(predictions, temperature)\n",
    "    preds = np.asarray(predictions).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def sample_model(model, seed, sample_length, int_to_atom, n_vocab, temperature, num_layers):\n",
    "    '''Prints out a sequence of text generated from the text data'''\n",
    "    # data = {data:[{poem,neurons}...],neurons}\n",
    "    feed_seq = seed #String of maxlen, or maxlen syllables\n",
    "    print('initialisation:', ''.join(feed_seq))\n",
    "    results = [].append(seed)\n",
    "    activations = [0]*len(seed) #no activations for seed\n",
    "    feed_seq = [atom_to_int[i] for i in feed_seq]\n",
    "    feed_seq = np_utils.to_categorical(feed_seq, num_classes=n_vocab)\n",
    "    \n",
    "    for _ in range(sample_length):\n",
    "        adict = get_activations(model,feed_seq)\n",
    "        preds = []\n",
    "        for k,v in adict.items():\n",
    "            if k[:4] == 'lstm':#this is interesting\n",
    "                \n",
    "            if k[:5] == 'dense': \n",
    "                preds = v\n",
    "        preds = #model.predict(feed_seq, batch_size=1)\n",
    "        predicted = pick_atom_index(preds, temperature)\n",
    "        results.append(int_to_atom[predicted])\n",
    "        \n",
    "        feed_seq = np.reshape(feed_seq, seq_length)\n",
    "        feed_seq = list(feed_seq)[1:] + [np.argmax(next_char)]\n",
    "    \n",
    "    return {'poem': results, 'neurons': activations}\n",
    "\n",
    "    \n",
    " start_index = random.randint(0, len(user) - maxlen - 1)\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = user[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "    \n",
    "sample_model(model, 25, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
