{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code is heavily inspired by these projects:\n",
    "- https://github.com/mathematiguy/keras-char-rnn\n",
    "- https://github.com/michaelrzhang/Char-RNN/\n",
    "- https://github.com/mineshmathew/char_rnn_karpathy_keras\n",
    "- https://github.com/ekzhang/char-rnn-keras\n",
    "- https://github.com/michaelrzhang/Char-RNN\n",
    "- https://github.com/yxtay/char-rnn-text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import slabikar\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from keras.utils import np_utils, Sequence\n",
    "from keras import backend as K\n",
    "from keract import get_activations\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 120 #length of sequence\n",
    "step = 13 #overlap\n",
    "validation_split = 0.1\n",
    "batch_size = 10\n",
    "rnn_size = 10#128\n",
    "num_layers = 3\n",
    "drop_prob = 0.1\n",
    "epochs = 6\n",
    "temperature=1.0\n",
    "sample_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Function for concatenating all text files from directory. Text files are expected to be utf-8 encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ''\n",
    "for filename in filter(lambda s: s.endswith(\".txt\"), os.listdir('resources/')):\n",
    "    print(\"loading file: %s\" % filename)\n",
    "    filepath = os.path.join('resources/', filename)\n",
    "    with open(filepath,'r', encoding='utf-8') as f:\n",
    "        text_data += f.read() + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is pretty small, we convert everything to lowercase and remove diacritics. There are some characters that need to be tweaked beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = text_data.replace(\"’\",\"\\'\")\n",
    "text_data = text_data.replace(\"„\",\"\\\"\")\n",
    "text_data = text_data.replace(\"“\",\"\\\"\")\n",
    "text_data = text_data.replace(\"‒\",\"-\")\n",
    "import unidecode\n",
    "text_data = unidecode.unidecode(text_data).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for processing texts. One uses syllables as text atoms, the other uses characters\n",
    "\n",
    "Return values:\n",
    "- atom_to_int: (dict) Maps characters in the character set to ints.\n",
    "- int_to_atom: (dict) Maps ints to characters in the character set.\n",
    "- n_atom: (int) The number of characters in the text.\n",
    "- n_vocab: (int) The number of unique characters in the text.'''\n",
    "- data: preprocessed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_char(text_data):\n",
    "    # create mapping of unique chars to integers, and a reverse mapping\n",
    "    chars = sorted(set(text_data)) #sorted is necessary for checkpointing model and reopening in later sessions\n",
    "    char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "    int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(text_data)\n",
    "    n_vocab = len(chars)    \n",
    "    return char_to_int, int_to_char, n_chars, n_vocab, text_data\n",
    "\n",
    "def process_text_syllable(text_data):\n",
    "    syllable_data = slabikar.slabikar(text_data)\n",
    "    syllables = sorted(set(syllable_data))\n",
    "    syllable_to_int = {c: i for i, c in enumerate(syllables)}\n",
    "    int_to_syllable = {i: c for i, c in enumerate(syllables)}\n",
    "    n_syllables = len(syllable_data)\n",
    "    n_vocab = len(syllables)    \n",
    "    return syllable_to_int, int_to_syllable, n_syllables, n_vocab, syllable_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes data to overlapping sequences. Since we are doing many to many RNN, targets are sequences of atoms shifted to the right from the source. Syllable data cannot be preprocessed as whole. Use generator instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInput(text, maxlen, step, n_vocab, atom_to_int):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    sentences = []\n",
    "    targets = []\n",
    "    for i in range(0, len(text) - maxlen - 1, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        targets.append(text[i + 1: i + maxlen + 1])\n",
    "    X = np.zeros((len(sentences), maxlen, n_vocab), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), maxlen, n_vocab), dtype=np.bool)\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        target = targets[i]\n",
    "        for j in range(maxlen):\n",
    "            X[i][j][atom_to_int[sentence[j]]] = 1\n",
    "            y[i][j][atom_to_int[target[j]]] = 1\n",
    "    return X,y\n",
    "\n",
    "def get_batch(batch, starts, text_data, seq_length, batch_size, \n",
    "              atom_to_int, n_vocab):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for start in range(batch_size * batch, batch_size * (batch + 1)): \n",
    "        seq_in  = text_data[starts[start]:starts[start] + seq_length]\n",
    "        seq_out = text_data[starts[start]+1:starts[start]+1 + seq_length]\n",
    "        dataX.append([atom_to_int[atom] for atom in seq_in])\n",
    "        dataY.append([atom_to_int[atom] for atom in seq_out])\n",
    "        \n",
    "    X = np_utils.to_categorical(dataX, num_classes=n_vocab)\n",
    "    y = np_utils.to_categorical(dataY, num_classes=n_vocab)\n",
    "    X = X.reshape(batch_size, seq_length, n_vocab) #might be unnecessary in this configuration\n",
    "    y = y.reshape(batch_size, seq_length, n_vocab)\n",
    "    return X, y\n",
    "\n",
    "#use this for syllables\n",
    "def generate_batches(mode, text_data, seq_length, validation_split,\n",
    "                     batch_size, atom_to_int, n_atom, n_vocab,\n",
    "                     random_seed=42, shuffle=True):\n",
    "    #Same seed is needed to guarantee that val. and train. sets are disjoint\n",
    "    random.seed(random_seed)\n",
    "    starts = list(range(n_atom - (seq_length+1)))\n",
    "    if shuffle:\n",
    "        random.shuffle(starts)\n",
    "    n_batches = len(starts) // batch_size\n",
    "    validation_size = round(n_batches * validation_split)\n",
    "    while True:\n",
    "        if mode == 'validation':\n",
    "            for batch in range(validation_size):\n",
    "                X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                                 batch_size, atom_to_int, n_vocab)\n",
    "                yield X, y\n",
    "            \n",
    "        elif mode == 'train':\n",
    "            for batch in range(validation_size, n_batches):\n",
    "                X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                                 batch_size, atom_to_int, n_vocab)\n",
    "                yield X, y\n",
    "        else:\n",
    "            raise ValueError(\"only 'validation' and 'train' modes accepted\")\n",
    "\n",
    "class BatchSequence(Sequence):\n",
    "    def __init__(self,mode, text_data, seq_length, validation_split,\n",
    "                     batch_size, atom_to_int, n_atom, n_vocab,\n",
    "                     random_seed=42, shuffle=True):\n",
    "        self.starts = list(range(0,n_atom - (seq_length+1)))\n",
    "        random.seed(random_seed)\n",
    "        if shuffle:\n",
    "            random.shuffle(self.starts)\n",
    "        self.n_batches = len(self.starts) // batch_size\n",
    "        self.validation_size = round(self.n_batches * validation_split)\n",
    "        self.mode = mode\n",
    "        self.text_data=text_data\n",
    "        self.atom_to_int=atom_to_int\n",
    "        self.n_vocab= n_vocab\n",
    "        self.seq_length=seq_length\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.mode == 'validation':\n",
    "            return self.validation_size            \n",
    "        elif self.mode == 'train':\n",
    "            return self.n_batches - self.validation_size\n",
    "        else:\n",
    "            raise ValueError(\"only 'validation' and 'train' modes accepted\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'validation':\n",
    "            X, y = get_batch(idx, self.starts, self.text_data, self.seq_length, \n",
    "                                 self.batch_size, self.atom_to_int, self.n_vocab)\n",
    "            return X, y\n",
    "            \n",
    "        elif self.mode == 'train':\n",
    "            X, y = get_batch(idx+self.validation_size, self.starts, self.text_data, self.seq_length, \n",
    "                                 self.batch_size, self.atom_to_int, self.n_vocab)\n",
    "            return X, y\n",
    "        else:\n",
    "            raise ValueError(\"only 'validation' and 'train' modes accepted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size, seq_length, n_vocab, rnn_size, num_layers, drop_prob):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            # add last layer\n",
    "            model.add(TimeDistributed(Dense(n_vocab, activation='softmax')))\n",
    "        elif i == 0:\n",
    "            # add first hidden layer\n",
    "            model.add(LSTM(rnn_size, batch_input_shape=(None, seq_length, n_vocab), return_sequences=True))\n",
    "            model.add(Dropout(drop_prob))\n",
    "        else:\n",
    "            # add middle hidden layer\n",
    "            model.add(LSTM(rnn_size, return_sequences=True))\n",
    "            model.add(Dropout(drop_prob))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use this to prepare char data (contains some debug printing to ensure everything looks good). CreateInput is risky to use with small step - large amount of RAM is used. Use Sequence instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atom_to_int, int_to_atom, n_atoms, n_vocab, data = process_text_char(text_data)\n",
    "X,y = createInput(data, maxlen, step, n_vocab, atom_to_int)\n",
    "print(atom_to_int)\n",
    "print(int_to_atom)\n",
    "print(n_atoms)\n",
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to prepare syllable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atom_to_int, int_to_atom, n_atoms, n_vocab, data = process_text_syllable(text_data)\n",
    "print(n_atoms)\n",
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train syllable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint('checkpoints/weights-{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
    "model = build_model(batch_size, maxlen, n_vocab, rnn_size, num_layers, drop_prob)\n",
    "\n",
    "# batches should divide sequences\n",
    "n_batches = (n_atoms - (maxlen+1)) // batch_size\n",
    "batch_params = (data, maxlen, validation_split, batch_size, atom_to_int, n_atoms, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    generator = BatchSequence('train', *batch_params),\n",
    "    validation_data = BatchSequence('validation', *batch_params),\n",
    "    validation_steps = int(n_batches * validation_split),\n",
    "    epochs = epochs, # tune following parameters at will. Steps per epoch are used for pure generator\n",
    "    steps_per_epoch = n_batches-1,\n",
    "    max_queue_size = 10,\n",
    "    workers=3,\n",
    "    use_multiprocessing=True,\n",
    "    verbose=1,\n",
    "    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'], label='training accuracy')\n",
    "plt.plot(history.history['val_acc'], label='validation accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can save the last model in case it hasn't improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('checkpoints/last.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can load saved model to play around with it (make sure to use correct dataset preprocessing for loaded model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('checkpoints/Fix-13-0.96-0.20.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training was using stateless model with fixed length sequence. This is not what we want to do since when generating, the sequence is theoretically unbounded length. Here is the function that converts a training model to one more suitable for generation. Using stateful model it should be possible to  and we want feed the model character after character but it seems that poems generated this way are confusing for the network and it outputs gibberish. So instead of this method (used in original Karpathy's article) we use computationally more intensive and less elegant approach. We use stateless network and feed it the whole poem during each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_model(model):\n",
    "    config = model.get_config()\n",
    "    \n",
    "    for layer in config['layers']:\n",
    "        if 'stateful' in layer['config']:\n",
    "            layer['config']['stateful'] = False #true for one char at time.\n",
    "        if 'trainable' in layer['config']:\n",
    "            layer['config']['trainable'] = False\n",
    "        if 'batch_input_shape' in layer['config']:\n",
    "            #we expect 3 dimensions\n",
    "            orig = layer['config']['batch_input_shape']\n",
    "            layer['config']['batch_input_shape'] = (1, None, orig[2]) #(None, 1, orig[2]) for one char at time\n",
    "    inference_model = Sequential.from_config(config)\n",
    "    inference_model.trainable = False\n",
    "    inference_model.set_weights(model.get_weights())\n",
    "    inference_model.reset_states()\n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel = generative_model(model)\n",
    "model.summary()\n",
    "gmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function chooses the atom output from prediction. The temperature controls the level of conservativeness of the network. Low temperature means the network chooses the highest scoring prediction, high temperature allows it to experiment more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_atom_index(predictions, temperature):\n",
    "    preds = np.asarray(predictions).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is used to generate output in the less elegant way with better results. (Requires small tweaks to work with char model - syllable model uses list of strings, char just one string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model_slow(model,sample_length, int_to_atom, atom_to_int, n_vocab, temperatures=[0.9]):\n",
    "    outerlist = []\n",
    "    for diversity in temperatures:\n",
    "        seed_string = slabikar.slabikar(\"vidim zem, jej jasot skvie sa \")\n",
    "        sl = len(seed_string)\n",
    "        for i in range(sample_length):\n",
    "            test_in = np.zeros((1, len(seed_string), n_vocab))\n",
    "            for t, atom in enumerate(seed_string):\n",
    "                test_in[0, t, atom_to_int[atom]] = 1\n",
    "            entire_prediction = gmodel.predict(test_in, verbose=0)[0] #batch wrapped\n",
    "            next_index = pick_atom_index(entire_prediction[-1], diversity)\n",
    "            next_atom = int_to_atom[next_index]\n",
    "            seed_string = seed_string + [next_atom]\n",
    "        print(\"\".join(seed_string))\n",
    "        #get activations for generated poem\n",
    "        last = np.zeros((1, len(seed_string), n_vocab))\n",
    "        for t, atom in enumerate(seed_string):\n",
    "            last[0, t, atom_to_int[atom]] = 1\n",
    "        adict = get_activations(model, last) \n",
    "        keylist = sorted(adict.keys()) #to guarantee that layers are in correct order all the time\n",
    "        neuronsSnapshot = [[] for i in range(len(seed_string))]# for each timestep we need to merge its layers\n",
    "        for k in keylist:\n",
    "            if 'lstm' in k:#this is interesting for us\n",
    "                for ix, act in enumerate(np.squeeze(adict[k])): # we are iterating over activations through time steps\n",
    "                    neuronsSnapshot[ix] = np.concatenate((neuronsSnapshot[ix], act))\n",
    "            else:\n",
    "                pass\n",
    "        outerlist.append({'poem': seed_string, 'neurons':np.array(neuronsSnapshot).tolist(), 'desc':''}) \n",
    "    return outerlist\n",
    "fout = sample_model_slow(gmodel, 10, int_to_atom, atom_to_int, n_vocab,temperatures=[0.6,0.9,1.2,1.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is used to generate some sample outputs from trained model in character by character fashion. It also saves activations of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(model,sample_length, int_to_atom, atom_to_int, n_vocab, temperatures):\n",
    "    outerlist = []\n",
    "    for diversity in temperatures:\n",
    "        print(\"generating with diversity: \"+str(diversity))\n",
    "        seed = [\"no\",\",\",\" \",\"na\",\" \",\"u\",\"pa\",\"tie\",\" \",\"vr\",\"chu\",\" \",\"pri\",\"duc\",\" \",\"za\"]\n",
    "        encoded = np.zeros((len(seed), n_vocab))\n",
    "        for t, atom in enumerate(seed):\n",
    "            encoded[t, atom_to_int[atom]] = 1.\n",
    "        #print(encoded)\n",
    "        generated = seed\n",
    "        #encoded = np_utils.to_categorical(seed_enc, num_classes=n_vocab)\n",
    "        model.reset_states()\n",
    "        neuronsSnapshot = []\n",
    "        for x in encoded[:-1]:\n",
    "            #print(x)\n",
    "            # input shape: (1, 1)\n",
    "            # saturate the net\n",
    "            model.predict([[[x]]])\n",
    "\n",
    "        next_index = encoded[-1]\n",
    "        for i in range(sample_length):\n",
    "            #print(i)\n",
    "            x = np.array([[next_index]])\n",
    "            # input shape: (1, 1)\n",
    "            #we need the activations\n",
    "            adict = get_activations(model, x)\n",
    "            keylist = sorted(adict.keys()) #to guarantee that layers are in correct all the time\n",
    "            preds = []\n",
    "            alist = []\n",
    "            for k in keylist:\n",
    "                if 'lstm' in k:#this is interesting\n",
    "                    #print(adict[k])\n",
    "                    alist = np.concatenate((alist, np.squeeze(adict[k])))\n",
    "                    #alist += adict[k] #seems like this is a numpy array\n",
    "                elif 'time' in k: \n",
    "                    probs = adict[k]\n",
    "                else:\n",
    "                    pass\n",
    "            #probs = model.predict(x)\n",
    "            # output shape: (1, 1, vocab_size)\n",
    "            number = pick_atom_index(probs[0][0], diversity)\n",
    "            # append to sequence\n",
    "            generated += int_to_atom[number]\n",
    "            neuronsSnapshot.append(alist)\n",
    "            next_index = np_utils.to_categorical(number,num_classes=n_vocab)\n",
    "        print(generated)\n",
    "        outerlist.append({'poem': generated[len(seed)-1:], 'neurons':np.array(neuronsSnapshot).tolist()})#-2 \n",
    "    return {'data': outerlist, 'neurons': len(outerlist[0]['neurons'][0])}\n",
    "    # data = {data:[{poem,neurons}...],neurons}\n",
    "\n",
    "\n",
    "    \n",
    "fout = sample_model(gmodel, 500, int_to_atom, atom_to_int, n_vocab,[0.5, 1.0, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the activations are saved for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('out/data.json','w') as f:\n",
    "    json.dump({\"data\":fout,\"neurons\":len(fout[0][\"neurons\"][0])}, f, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small tool to visually check how is the network doing so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BatchSequence('validation', *batch_params, shuffle=False)\n",
    "print(\"_________\")\n",
    "X,y =  g.__getitem__(1)\n",
    "print(X[0])\n",
    "preds = model.predict(X, verbose=1)\n",
    "print(list(np.argmax(y[0],axis=1)))\n",
    "print(\"\".join(list(map(lambda x: int_to_atom[x],list(np.argmax(preds[0],axis=1))))))\n",
    "print(\"_________\")\n",
    "print(\"\".join(list(map(lambda x: int_to_atom[x],list(np.argmax(preds[2],axis=1))))))\n",
    "print(\"_________\")\n",
    "print(\"\".join(list(map(lambda x: int_to_atom[x],list(np.argmax(preds[3],axis=1))))))\n",
    "print(\"_________\")\n",
    "print(\"\".join(list(map(lambda x: int_to_atom[x],list(np.argmax(preds[4],axis=1))))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
